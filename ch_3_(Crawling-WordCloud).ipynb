{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 크롤링(Web Crawling)\n",
    "## 1. 크롤링(crawoling) 이란?\n",
    "- Web상에 존재하는 Contents를 수집하는 작업 (프로그래밍으로 자동화 가능)\n",
    "    - HTML 페이지를 가져와서, HTML/CSS등을 파싱하고, 필요한 데이터만 추출하는 기법\n",
    "    - Open API(Rest API)를 제공하는 서비스에 Open API를 호출해서, 받은 데이터 중 필요한 데이터만 추출하는 기법\n",
    "    - Selenium등 브라우저를 프로그래밍으로 조작해서, 필요한 데이터만 추출하는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BeautifulSoup 크롤링\n",
    "- HTML의 태그를 파싱해서 필요한 데이터만 추출하는 함수를 제공하는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1) reqeusts 라이브러리를 활용한 HTML 페이지 요청 \n",
    "# 1-1) res 객체에 HTML 데이터가 저장되고, res.content로 데이터를 추출할 수 있음\n",
    "res = requests.get('https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=001&aid=0011825430')\n",
    "\n",
    "# print(res.content)\n",
    "# 2) HTML 페이지 파싱 BeautifulSoup(HTML데이터, 파싱방법)\n",
    "# 2-1) BeautifulSoup 파싱방법\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "# 3) 필요한 데이터 검색\n",
    "title = soup.find('title')\n",
    "\n",
    "# 4) 필요한 데이터 추출\n",
    "print(title.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup 기초 함수(find, find_all)\n",
    "- find() : 가장 먼저 검색되는 태그 반환\n",
    "- find_all() : 전체 태그 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1 id='title'>[1]크롤링이란?</h1>;\n",
    "        <p class='cssstyle'>웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
    "        <p id='body' align='center'>파이썬을 중심으로 다양한 웹크롤링 기술 발달</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_data = soup.find('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title_data.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title_data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 먼저 검색되는 태그를 반환\n",
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태그에 있는 id로 검색 (javascript 예를 상기!)\n",
    "soup.find(id='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML 태그와 CSS class를 활용해서 필요한 데이터를 추출하는 방법1\n",
    "soup.find('p', class_='cssstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML 태그와 CSS class를 활용해서 필요한 데이터를 추출하는 방법2\n",
    "soup.find('p', 'cssstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML 태그와 태그에 있는 속성:속성값을 활용해서 필요한 데이터를 추출하는 방법\n",
    "soup.find('p', attrs = {'align': 'center'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all() 관련된 모든 데이터를 리스트 형태로 추출하는 함수\n",
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- string 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('http://v.media.daum.net/v/20170518153405933')\n",
    "soup = BeautifulSoup(res.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all(string='오대석'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all(string=['[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI', '오대석']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all(string='AI'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 30대 남성 뉴스 랭킹\n",
    "- https://news.daum.net/ranking/age/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://news.daum.net/ranking/age/')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.find(string='30대 남성')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.find_parent('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_title = text.find_all('a','link_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find_all() 메서드를 사용해서 태그와 클래스이름으로 링크가 걸려있는 기사 타이틀을 가져오기\n",
    "for num in range(len(link_title)):\n",
    "    print(link_title[num].get_text().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup CSS Selector 사용한 크롤링\n",
    "- CSS 선택 문법을 이용하여 태그 검색\n",
    "- select 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://news.v.daum.net/v/20200819204821138')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태그 검색\n",
    "soup.find('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 함수는 리스트 형태로 전체 반환\n",
    "soup.select('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기가 있다면 하위 태그를 검색\n",
    "soup.select('html head title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기가 있다면 하위 태그를 검색\n",
    "# 이때 바로 직계의 자식이 아니여도 관계없음\n",
    "soup.select('html title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > 를 사용하는 경우 바로 아래의 자식만 검색\n",
    "# 바로 아래 자식이 아니기 때문에 에러 발생\n",
    "soup.select('html > title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바로 아래 자식을 검색\n",
    "soup.select('head > title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .은 태그의 클래스를 검색\n",
    "# class가 article_view인 태그 탐색\n",
    "soup.select('.article_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# div태그 중 class가 article_view인 태그 탐색\n",
    "soup.select('div.article_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# div 태그 중 id가 harmonyContainer인 태그 탐색\n",
    "soup.select('#harmonyContainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# div태그 중 id가 mArticle 인 태그의 하위 태그 중 아이디가 article_title인 태그\n",
    "soup.select('div#mArticle div#harmonyContainer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSS Selector 구하기 -> Chrome Browser 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('#harmonyContainer > section')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 여러 페이지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://news.daum.net/ranking/age/')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.find(string='30대 남성')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.find_parent('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_title = text.find_all('a','link_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " a = link_title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_attribute_list('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_contents=[]\n",
    "for item in link_title:\n",
    "    sub_res = requests.get(item.get_attribute_list('href')[0])\n",
    "    sub_soup = BeautifulSoup(sub_res.content, 'html.parser')\n",
    "    news_contents.append(sub_soup.select('#harmonyContainer > section')[0].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selenium 웹 크롤링\n",
    "- 브라우저를 제어해서 크롤링을 하는 방법\n",
    "- Selenium: 웹을 테스트하기 위한 프레임워크\n",
    "- 공식 홈페이지(http://www.seleniumhq.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전준비 (Selenium 설치)\n",
    "1. Selenium 인스톨: pip install selenium\n",
    "2. 웹드라이버 인스톨: 웹 테스트 자동화를 위해 제공되는 툴(각 browser및 os 별로 존재)\n",
    "- selenium - 테스트 코드를 사용하여 브라우져에서의 액션을 테스트할 수 있게 해주는 툴\n",
    "- Firefox, chromedriver 등 각 브라우져마다 웹드라이버 다운로드 가능\n",
    "+ https://sites.google.com/a/chromium.org/chromedriver/ (Chrome 브라우저용)\n",
    "+ https://github.com/mozilla/geckodriver/releases (Firefox 브라우저용)\n",
    "\n",
    "확인: 설치 디렉토리를 알아두어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    !pip install selenium\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# 드라이버 생성\n",
    "# chromedriver 설치된 경로를 정확히 기재해야 함\n",
    "chromedriver = 'D:/Downloads/chromedriver.exe' # 윈도우 \n",
    "#chromedriver = '/usr/local/Cellar/chromedriver/chromedriver' # 맥\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "# 크롤링할 사이트 호출\n",
    "driver.get(\"http://www.python.org\")\n",
    "\n",
    "# Selenium은 웹테스트를 위한 프레임워크로 다음과 같은 방식으로 웹테스트를 자동으로 진행함 (참고)\n",
    "assert \"Python\" in driver.title\n",
    "\n",
    "# <input id=\"id-search-field\" name=\"q\" 검색창 name으로 검색하기\n",
    "# 태그 name으로 특정한 태그를 찾을 수 있음\n",
    "elem = driver.find_element_by_name(\"q\")\n",
    "\n",
    "# input 텍스트 초기화 \n",
    "# elem.clear()\n",
    "\n",
    "# 키 이벤트 전송가능함\n",
    "# 태그가 input 태그이므로 입력창에 키이벤트가 전달되면, 입력값이 자동으로 작성됨\n",
    "elem.send_keys(\"pycon\")\n",
    "\n",
    "# 태그가 input 태그이므로 엔터 입력시 form action이 진행됨\n",
    "elem.send_keys(Keys.RETURN)\n",
    "\n",
    "# Selenium은 웹테스트를 위한 프레임워크로 다음과 같은 방식으로 웹테스트를 자동으로 진행함 (참고)\n",
    "assert \"No results found.\" not in driver.page_source\n",
    "\n",
    "# 명시적으로 일정시간을 기다릴 수 있음 (10초 기다림)\n",
    "time.sleep(10)\n",
    "\n",
    "# 크롬 브라우저 닫기 가능함\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open API 활용 Naver 검색\n",
    "- Naver 개발자 등록하기 : https://developers.naver.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "client_id = 'gzzaTJH1zQcv3qtEeVhf'\n",
    "client_secret = 'wFk4b9RBO0'\n",
    "\n",
    "# 한글등 non-ASCII text를 URL에 넣을 수 있도록 \"%\" followed by hexadecimal digits 로 변경\n",
    "# URL은 ASCII 인코딩셋만 지원하기 때문임\n",
    "encText = urllib.parse.quote_plus(\"국내 여행\")\n",
    "# print(encText)\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText # json 결과\n",
    "# url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # xml 결과\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode==200):\n",
    "    response_body = response.read()\n",
    "    print(response_body.decode('utf-8'))\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText + \"&display=100\"# json 결과\n",
    "# url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # xml 결과\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode==200):\n",
    "    response_body = response.read()\n",
    "    data = json.loads(response_body)\n",
    "    print(data['items'])\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText + \"&display=100&start=101\"# json 결과\n",
    "# url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # xml 결과\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode==200):\n",
    "    response_body = response.read()\n",
    "    data = json.loads(response_body)\n",
    "    print(data['items'])\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_max = 1001\n",
    "datas = []\n",
    "\n",
    "start_item_num = 1\n",
    "\n",
    "while(start_item_num < start_max):\n",
    "    url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText + \"&display=100&start=\" + str(start_item_num)\n",
    "    # url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # xml 결과\n",
    "    print(url)\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "    if(rescode==200):\n",
    "        response_body = response.read()\n",
    "        data = json.loads(response_body)\n",
    "        datas.extend(data['items'])\n",
    "        #print(data)\n",
    "    else:\n",
    "        print(\"Error Code:\" + rescode)\n",
    "    \n",
    "    start_item_num += 100\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%9D%B8%EA%B5%AC%EC%88%9C_%EB%8F%84%EC%8B%9C_%EB%AA%A9%EB%A1%9D')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_html(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df = tables[0]\n",
    "city_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_name(row):\n",
    "    if(' ' in row['행정구역']):\n",
    "        row['행정구역'] = row['행정구역'].split(' ')[1]\n",
    "    return row['행정구역']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df.apply(get_city_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_name(row):\n",
    "    res = row['행정구역']\n",
    "    if(' ' in res):\n",
    "        res = res.split(' ')[1]\n",
    "    if('특별자치시' in res):\n",
    "        res = res.replace('특별자치시','')\n",
    "    if('특별시' in res):\n",
    "        res = res.replace('특별시','')\n",
    "    if('광역시' in res):\n",
    "        res = res.replace('광역시','')\n",
    "    if('시' in res[-1:]):\n",
    "        res = res[:-1]\n",
    "    if('군' in res[-1:]):\n",
    "        res = res[:-1]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = city_df.apply(get_city_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_list = list(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[df['title'].str.contains('|'.join(cities_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city'] = [x[0] if len(x)> 0 else '' for x in df['title'].str.findall('|'.join(cities_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[df['city'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = new_df.groupby(['city']).count()[['title']].reset_index()\n",
    "new_df2.columns = ['city','count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "sns.barplot(x='city',y='count',data=new_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "sns.barplot(x='city',y='count',data=new_df2)\n",
    "\n",
    "plt.xticks(\n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df3 = new_df2.sort_values('count', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "chart = sns.barplot(x='city',y='count',data=new_df3)\n",
    "plt.xticks(\n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open API 활용 Twitter 검색\n",
    "- 트위터 개발자 등록하기 : https://developer.twitter.com/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# 트위터 API 개발자 키를 아래에 입력\n",
    "client_key = 'zmqfKGreKX4kTeoFnVmydvHvE'\n",
    "client_secret = 'y7v6WRI3Fauu35eiUhO7K9Myg1Jh9ZoLg9L9miVtmmGAWg9nTC'\n",
    "\n",
    "# b64 encoded 형태로 만드는 과정 \n",
    "key_secret = '{}:{}'.format(client_key, client_secret).encode('ascii')\n",
    "b64_encoded_key = base64.b64encode(key_secret)\n",
    "b64_encoded_key = b64_encoded_key.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# request에 필요한 url 만들기\n",
    "base_url = 'https://api.twitter.com/'\n",
    "auth_url = '{}oauth2/token'.format(base_url)\n",
    "\n",
    "# HEADER 구성하기\n",
    "auth_headers = {\n",
    "    'Authorization': 'Basic {}'.format(b64_encoded_key),\n",
    "    'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'\n",
    "}\n",
    "\n",
    "# Authentication Data section 만들기\n",
    "auth_data = {\n",
    "    'grant_type': 'client_credentials'\n",
    "}\n",
    "\n",
    "# POST request를 보내서 status 확인!\n",
    "auth_resp = requests.post(auth_url, headers=auth_headers, data=auth_data)\n",
    "print(auth_resp.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bearer token 정의하기\n",
    "access_token = auth_resp.json()['access_token']\n",
    "\n",
    "# Search HEADER 구성하기\n",
    "search_headers = {\n",
    "    'Authorization': 'Bearer {}'.format(access_token)\n",
    "}\n",
    "\n",
    "# SEARCH TWEET\n",
    "# Maximum number of tweets returned from a single token is 18,000 \n",
    "\n",
    "search_params = {\n",
    "    'q':'여행',\n",
    "    'result_type': 'popular', # 'mixed', recent', 'popular' \n",
    "    'count':100, #  15 - 100\n",
    "    'retryonratelimit':True, # rate limit에 도달했을 때 자동으로 다시 trial\n",
    "}\n",
    "\n",
    "search_url = '{}1.1/search/tweets.json'.format(base_url)\n",
    "search_resp = requests.get(\n",
    "    search_url, headers=search_headers, \n",
    "    params=search_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "data = json.loads(search_resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data['statuses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tot_data = []\n",
    "old_max_id = 1296368293448622082\n",
    "\n",
    "for x in range(50):\n",
    "    search_params = {\n",
    "        'q':'여행',\n",
    "        'result_type': 'recent', # 'mixed', recent', 'popular' \n",
    "        'count':100, #  15 - 100\n",
    "        'retryonratelimit':True, # rate limit에 도달했을 때 자동으로 다시 trial\n",
    "        'max_id' : old_max_id\n",
    "    }\n",
    "\n",
    "    search_url = '{}1.1/search/tweets.json'.format(base_url)\n",
    "    search_resp = requests.get(\n",
    "        search_url, headers=search_headers, \n",
    "        params=search_params\n",
    "    )\n",
    "    \n",
    "    data = json.loads(search_resp.content)\n",
    "    \n",
    "    old_max_id = data['statuses'][-1]['id']-1\n",
    "\n",
    "    tot_data.extend(data['statuses'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_data_df = pd.DataFrame(tot_data)\n",
    "tot_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tweepy 라이브러리로 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "#트위터의 개인 앱 계정에서 아래 4가지 사항 확인\n",
    "consumer_key = \"zmqfKGreKX4kTeoFnVmydvHvE\"\n",
    "consumer_secret = \"y7v6WRI3Fauu35eiUhO7K9Myg1Jh9ZoLg9L9miVtmmGAWg9nTC\"\n",
    "access_token = \"951618999405617152-bC4EbM8Q5x0NFpqn7w0PJmGiKsNLOvN\"\n",
    "access_token_secret = \"8ouskEQZp1Le5ZaQrCKYc39MdV5mf4dUxoR9f0JFzB9Zl\"\n",
    "\n",
    "\n",
    "#계정 승인\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "twitter_api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "#검색 키워드 정의\n",
    "keyword = \"여행\"\n",
    "api_result = []\n",
    "\n",
    "\n",
    "#키워드 검색 및 결과\n",
    "tweet = twitter_api.search(keyword)\n",
    "for tw in tweet:\n",
    "    api_result.append(tw.text) #텍스트 결과만 담기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GetOldTweet3 라이브러리로 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install GetOldTweets3\n",
    "import GetOldTweets3 as got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "days_range = []\n",
    "\n",
    "start = datetime.datetime.strptime(\"2020-08-01\", \"%Y-%m-%d\")\n",
    "end = datetime.datetime.strptime(\"2020-08-10\", \"%Y-%m-%d\")\n",
    "date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]\n",
    "\n",
    "for date in date_generated:\n",
    "    days_range.append(date.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 수집 기간 맞추기\n",
    "start_date = days_range[0]\n",
    "end_date = (datetime.datetime.strptime(days_range[-1], \"%Y-%m-%d\") \n",
    "            + datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\") # setUntil이 끝을 포함하지 않으므로, day + 1\n",
    "\n",
    "# 트윗 수집 기준 정의\n",
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch('국내')\\\n",
    "                                           .setSince(start_date)\\\n",
    "                                           .setUntil(end_date)\\\n",
    "                                           .setMaxTweets(-1)\n",
    "\n",
    "# 수집 with GetOldTweet3\n",
    "print(\"Collecting data start.. from {} to {}\".format(days_range[0], days_range[-1]))\n",
    "start_time = time.time()\n",
    "\n",
    "tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "print(\"Collecting data end.. {0:0.2f} Minutes\".format((time.time() - start_time)/60))\n",
    "print(\"=== Total num of tweets is {} ===\".format(len(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# initialize\n",
    "tweet_list = []\n",
    "\n",
    "for index in tqdm.tqdm_notebook(tweet):\n",
    "    \n",
    "    # 메타데이터 목록 \n",
    "    username = index.username\n",
    "    link = index.permalink \n",
    "    content = index.text\n",
    "    tweet_date = index.date.strftime(\"%Y-%m-%d\")\n",
    "    tweet_time = index.date.strftime(\"%H:%M:%S\")\n",
    "    retweets = index.retweets\n",
    "    favorites = index.favorites\n",
    "     \n",
    "    # 결과 합치기\n",
    "    info_list = [tweet_date, tweet_time, username, content, link, retweets, favorites]\n",
    "                 #joined_date, num_tweets, num_following, num_follower]\n",
    "    tweet_list.append(info_list)\n",
    "    \n",
    "    # 휴식 \n",
    "    time.sleep(uniform(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = pd.DataFrame(tweet_list, \n",
    "                          columns = [\"date\", \"time\", \"user_name\", \"text\", \"link\", \"retweet_counts\", \"favorite_counts\"])\n",
    "twitter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud 만들어보기\n",
    "- JDK 설치 -> JAVA_HOME 설정\n",
    "- JPype 설치\n",
    "- konlpy 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install JPype1-1.0.2-cp38-cp38-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 로드\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "base_url = 'https://news.daum.net/breakingnews/digital'\n",
    "\n",
    "articles = []\n",
    "max_count = 100\n",
    "page_no = 1\n",
    "while len(articles) < max_count:\n",
    "    page_url = '{}?page={}'.format(base_url, page_no)\n",
    "    page_response = requests.get(page_url)\n",
    "    page_soup = BeautifulSoup(page_response.text)\n",
    "    article_links = page_soup.select('div.cont_thumb a.link_txt')\n",
    "    for link in article_links:\n",
    "        article_url = link.get('href')\n",
    "        if 'http' not in article_url:\n",
    "            continue\n",
    "        article_response = requests.get(article_url)\n",
    "        article_soup = BeautifulSoup(article_response.text)\n",
    "        article_text = article_soup.select('div.article_view')[0].text\n",
    "        articles.append(article_text.strip().replace('\\n', ' '))\n",
    "    \n",
    "    print(len(articles))\n",
    "    page_no += 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 갯수 세기\n",
    "from konlpy.tag import Komoran\n",
    "word_counts = {}\n",
    "\n",
    "komoran = Komoran()\n",
    "for article_text in articles:\n",
    "    nouns = komoran.nouns(article_text)\n",
    "    for noun in nouns:\n",
    "        if len(noun) == 1:     # 한글자 명사 제외\n",
    "            continue\n",
    "        if noun not in word_counts:\n",
    "            word_counts[noun] = 0\n",
    "        word_counts[noun] += 1\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# wordcloud 생성기 객체 만들고, word_counts 로 부터 wordcloud 생성\n",
    "wc = WordCloud(\n",
    "    font_path='NanumGothic.ttf',\n",
    "    background_color='white',\n",
    "    width=800,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "wc_img = wc.generate_from_frequencies(word_counts)\n",
    "file_name = 'my_first_wordcloud.jpg'\n",
    "#wc_img.to_file(file_name)   # 이미지 파일 저장\n",
    "\n",
    "# show\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.imshow(wc_img, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
